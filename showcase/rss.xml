<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Showcase on Matt Copperwaite</title><link>https://matt.copperwaite.net/showcase/</link><description>Recent content in Showcase on Matt Copperwaite</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><managingEditor>matt@copperwaite.net (Matt Copperwaite)</managingEditor><webMaster>matt@copperwaite.net (Matt Copperwaite)</webMaster><copyright>CC BY-SA 4.0</copyright><atom:link href="https://matt.copperwaite.net/showcase/rss.xml" rel="self" type="application/rss+xml"/><item><title>This Blog</title><link>https://matt.copperwaite.net/showcase/this-blog/</link><pubDate>Wed, 10 Mar 2021 18:50:32 +0100</pubDate><author>matt@copperwaite.net (Matt Copperwaite)</author><guid>https://matt.copperwaite.net/showcase/this-blog/</guid><description>&lt;p>&lt;a href="https://matt.copperwaite.net">This blog&lt;/a> is my attempt at consolidating my IT security knowledge using Agile methodology.&lt;/p>
&lt;p>I had a couple of requirements:&lt;/p>
&lt;ul>
&lt;li>Cost sensitive - When it comes to personal projects I try to keep costs as low as possible&lt;/li>
&lt;li>No Analytics - I appreciate it if people read my blog, but I&amp;rsquo;m mainly writing for myself. I don&amp;rsquo;t care in detail who reads it or how they interact with it.&lt;/li>
&lt;li>Minimal bandwidth costs - I don&amp;rsquo;t want this blog to go down if I have the fortunate problem of becoming popular&lt;/li>
&lt;li>Static site - I knew I wanted to play with a static site generator&lt;/li>
&lt;li>Use of Content linting tools - I wanted to ensure the quality of the content of my blog by using a pipeline that measured content quality.&lt;/li>
&lt;li>Create content from mobile - So if I have a few spare minutes I can throw something together&lt;/li>
&lt;/ul>
&lt;p>I really enjoy this sort of thinking. It initially left me one option a public S3 bucket, however because the S3 costs are not zero I spent time thinking about it more and realised the tool that filled most of these options was GitHub Pages.&lt;/p>
&lt;p>Now GitHub Pages does have its own static page generator, but after a bit of research I settled on Hugo as being the most widely used as well as being the best documented.&lt;/p>
&lt;p>One thing that didn&amp;rsquo;t work out was being able to edit content from a phone. It turns out all the GitHub Android apps are pretty terrible and out-dated and editing files from them isn&amp;rsquo;t really possible. Ah well. Can&amp;rsquo;t win them all.&lt;/p>
&lt;p>That is not to say my journey in to Hugo was fun, things like using submodules in git makes me squirm, but I&amp;rsquo;m super happy with the results. I&amp;rsquo;ve also set it up in a way which means if I wanted to set up other blogs it would be very little effort to get it going.&lt;/p>
&lt;h2 id="content-quality">Content Quality&lt;/h2>
&lt;p>I used to have (or more accurately abandoned) a Twitter account after seeing the Twitter backlash and decided I didn&amp;rsquo;t feel comfortable with my own writing. One of the things that concerned me is my ability to explain complex problems in a concise way. I saw a lot of people revert to blogs when Twitter didn&amp;rsquo;t fill the gap, so I figured a blog was the best answer. I also wanted to minimize my own biases in my writing and wanted to be as inclusive as possible with the aim to minimise backlash. To me the only way of doing that was to bring in content quality tooling in much the same way you would bring in linters and tests to bring in code quality, seemed the best way of doing that.&lt;/p>
&lt;p>After some discussions with colleagues I gathered some tooling that I wanted to incline in a CI/CD pipeline. I don&amp;rsquo;t think this is the extent of tools that could be included but enough tooling to make me comfortable with writing a blog.&lt;/p>
&lt;h3 id="alexjs">AlexJS&lt;/h3>
&lt;p>Using GitHub Actions allowed me to inline the assessment of the quality of my content. One of the tools I chose was &lt;a href="https://alexjs.com/">AlexJS&lt;/a>. I was already a little familiar with it anyway, having been referred it by a friend. AlexJS looks to improve the equality in your use of language and is super cool and it&amp;rsquo;s been fascinating how it&amp;rsquo;s been teaching me to adapt my language. I looked for similar products that I could also include, but I wasn&amp;rsquo;t very successful.&lt;/p>
&lt;h3 id="make-words-better">Make words better&lt;/h3>
&lt;p>Another things I did want to include was something like Grammarly to improve the understanding of the content, however as it was not open source and the API didn&amp;rsquo;t seem to meet what I wanted to do I discovered GrammarBot. It still didn&amp;rsquo;t integrate nicely with CI/CD pipelines, so &lt;a href="https://github.com/yamatt/python3-hemoglobin">I wrote a wrapper&lt;/a> to print human readable results.&lt;/p>
&lt;h3 id="reading-complexity-and-length">Reading complexity and length&lt;/h3>
&lt;p>The other thing I wanted to do to improve content quality was to ensure that when writing about complex topics I was not alienating anyone and that the reading length is not too long. I really struggled to find many tools here. The one I found is a Python library called textstat, which reads plain text files and derives lots of stats about complexity. I haven&amp;rsquo;t taken the time to study in detail what the numbers mean, but I figured if the numbers were at least reasonably consistent I&amp;rsquo;m probably doing OK.&lt;/p>
&lt;p>However, since textstat is a library I needed to &lt;a href="https://github.com/yamatt/python3-textstat-cli">write a wrapper&lt;/a> that allowed it to be included in a CI/CD pipeline and be human readable.&lt;/p>
&lt;p>The other thing I wanted to do was to ensure the reading length of the text was no more than 10 minutes. I use Firefox&amp;rsquo;s Reader view quite a lot and at the top it gives you an approximate reading time. My upper bound on that is 10 minutes. It&amp;rsquo;s partly a free time thing, but also I feel like if I can&amp;rsquo;t explain a problem within a reasonable time-frame I need to break it out in to separate posts that explain a specific problem in more detail.&lt;/p>
&lt;p>This reading length requirement is actually a really quick bit of maths to detect the number of words and then divide that by a variety of people&amp;rsquo;s reading speeds. It was so quick to do I actually bundled it in to &lt;a href="https://github.com/yamatt/python3-textstat-cli">textstat-cli&lt;/a>.&lt;/p>
&lt;h2 id="future">Future&lt;/h2>
&lt;h3 id="stats">Stats&lt;/h3>
&lt;p>One day, once I get enough posts together I will try to see what interesting stats I can find. Since I&amp;rsquo;m not using analytics I&amp;rsquo;ll never be able to do that end of year blog post that my favourite blog posts do that usually discuss their most popular blog posts. But perhaps for me that will be the time I do my stats reviews.&lt;/p>
&lt;h3 id="comments-on-posts">Comments on Posts&lt;/h3>
&lt;p>I did want to include comments on blog posts as a proxy for page impressions and to improve engagement. I played with using GitHub Issues as comments and as you might expect I&amp;rsquo;m &lt;a href="https://github.com/krasimir/octomments">far from alone&lt;/a> in that. However I am yet to have the time to include it.&lt;/p>
&lt;h3 id="link-validation">Link Validation&lt;/h3>
&lt;p>Another thing I&amp;rsquo;m thinking of doing is to also validate any links I use in my posts. If I link to something that page needs to exist, and be stored in the Way Back Machine. This is more difficult to achieve than I was expecting, so something for another time.&lt;/p></description></item><item><title>CharDiff</title><link>https://matt.copperwaite.net/showcase/chardiff/</link><pubDate>Mon, 30 Nov 2020 10:24:32 +0100</pubDate><author>matt@copperwaite.net (Matt Copperwaite)</author><guid>https://matt.copperwaite.net/showcase/chardiff/</guid><description>&lt;p>&lt;a href="https://github.com/yamatt/python3-chardiff">CharDiff&lt;/a> is an easy-to-use tool to compare 2 strings and highlight the characters in them that are different.&lt;/p>
&lt;p>This came about for a couple reasons.&lt;/p>
&lt;p>For one I had some very long strings I was working with at the time and I was having a lot of difficulty finding where they differed. I really struggled to find any tools that would allow me to find the difference between strings at a character level.&lt;/p>
&lt;p>The other reason was I was looking for a very quick project in Python3 that I could use to do things properly. By which I mean, have tested code, have it running in a CI/CD pipeline (I wanted to play with the GitHub Actions beta I&amp;rsquo;d got myself on to) and have it publish to PyPI.&lt;/p>
&lt;p>I even already have some &lt;a href="https://github.com/yamatt/python3-chardiff/issues/2">nice feedback from some users&lt;/a>.&lt;/p>
&lt;p>I have seen subsequent posts around the web of tools like this, and there are &lt;a href="https://github.com/johannhof/difference.rs">definitely better ones out there&lt;/a>.&lt;/p>
&lt;p>But it wasn&amp;rsquo;t to be the best but to expand my knowledge. I now have a template for future projects.&lt;/p></description></item><item><title>TV Show Chart</title><link>https://matt.copperwaite.net/showcase/tv-show-chart/</link><pubDate>Sat, 28 Nov 2020 17:14:32 +0100</pubDate><author>matt@copperwaite.net (Matt Copperwaite)</author><guid>https://matt.copperwaite.net/showcase/tv-show-chart/</guid><description>&lt;p>&lt;a href="https://yamatt.github.io/tv-show-chart/">TV Show Chart&lt;/a> allows you to visualise the popularity of TV Show episodes across seasons using IMDb data and GitHub Actions.&lt;/p>
&lt;p>There used to be a website that did this for you. It seemed to go away. So I decided to write some quick hacks to make my own. It is really bare minimum to get it working.&lt;/p>
&lt;p>It started off being nothing good or professional about the code behind the repo. I am not a front-end person at all, other than I like doing a bit of JavaScript from time to time.&lt;/p>
&lt;p>I personally prefer the IMDb popularity scores, but their API is functionally non-existent. So I wrote some Python to pull all the code together.&lt;/p>
&lt;p>Originally my plan was to store the episode data objects in maybe S3 or B2 and process the data in AWS Lambdas.&lt;/p>
&lt;p>However I could never get the Lambdas to complete in time and I was worried if the site became popular I would be paying for bandwidth.&lt;/p>
&lt;p>I then realised I might be able to store the data in GitHub, but the data as JSON was far too big. I seem to remember reading that there is a 2Gb limit on GitHub repo sizes. So I created my own proprietary format to store the data in a more succinct way.&lt;/p>
&lt;p>Recently I&amp;rsquo;ve been playing with static sites in GitHub and these work really well with GitHub Actions, commiting the code back to the repo.&lt;/p>
&lt;p>Plus I read an article about grabbing &lt;a href="https://coviddata.github.io/coviddata/">Covid data using GitHub Actions on a schedule&lt;/a>. It really made me want to revisit this work since the data hadn&amp;rsquo;t been updated in months.&lt;/p>
&lt;p>I also found that GitHub Actions are way more powerful that Lambdas so I could run the scripts in a matter of minutes, which was nice.&lt;/p>
&lt;p>There are definitely some UX improvements I would like to make so it&amp;rsquo;s possible to search for TV shows, but I need to do a bit of professionalisation first.&lt;/p></description></item><item><title>Filter Annoyances</title><link>https://matt.copperwaite.net/showcase/filter-annoyances/</link><pubDate>Fri, 27 Nov 2020 17:14:32 +0100</pubDate><author>matt@copperwaite.net (Matt Copperwaite)</author><guid>https://matt.copperwaite.net/showcase/filter-annoyances/</guid><description>&lt;p>&lt;a href="https://yamatt.github.io/filter-annoyances/">Filter Annoyances&lt;/a> is some filters for uBlock Origin that I did not find in other filter lists.&lt;/p>
&lt;p>This is my first foray in to making uBlock Origin. I was at the time, and still do, wanted to remove the cookie and GDPR pop-ups but still use the site. These filters help facilitate that.&lt;/p>
&lt;p>There are a couple of built-in ones to uBlock such as &lt;a href="https://github.com/easylist/easylist">easylist cookie&lt;/a> and uBlock Annoyances, but these looked difficult to contribute to.&lt;/p>
&lt;p>I did use &lt;a href="https://addons.mozilla.org/en-GB/firefox/addon/i-dont-care-about-cookies/">&lt;em>I don&amp;rsquo;t care about cookies&lt;/em>&lt;/a> for a little while, which worked fine, but I couldn&amp;rsquo;t find it&amp;rsquo;s source code anywhere, despite it saying it&amp;rsquo;s open source. So I thought to be safe I would stop using it.&lt;/p>
&lt;p>I&amp;rsquo;m kind of curious about ways you can test and validate uBlock Origin filters. It seems kind of difficult.&lt;/p>
&lt;p>I also found that the &lt;a href="https://github.com/gorhill/uBlock/wiki/Static-filter-syntax">filter syntax used in uBlock Origin comes from Adblock Plus&lt;/a>, and they can be very imprecise. But the uBlock Origin element picker tool is amazing. Some of the time I use it then copy the generated rule in to this repo.&lt;/p>
&lt;p>I chose (rather unusually for me) the CC0 license for these filters because I wanted to put them as much in to the public domain as possible, and because they are not really programmatic scripts.&lt;/p>
&lt;p>The distractions list is purely for things that take me away being productive on sites I use day-to-day. Kind of dark patterns really that keep you on their site.&lt;/p>
&lt;p>The aggressive filter I&amp;rsquo;ve moved away from because it was too difficult to maintain and I kept having to switch off uBlock to see if the page worked, which then meant I saw ads.&lt;/p></description></item></channel></rss>